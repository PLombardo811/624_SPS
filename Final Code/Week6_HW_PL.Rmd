---
title: "Week6_HW_PL"
author: "Peter Lombardo"
date: "March 9, 2019"
output: html_document
---
```{r}
suppressMessages(suppressWarnings(library(readxl)))
suppressMessages(suppressWarnings(library(ggplot2)))
suppressMessages(suppressWarnings(library(forecast)))
suppressMessages(suppressWarnings(library(fpp2)))
suppressMessages(suppressWarnings(library(mlbench)))
suppressMessages(suppressWarnings(library(caret)))
suppressMessages(suppressWarnings(library(caTools)))
```

#8.1: Figure 8.31 shows the ACFs for 36 random numbers, 360 random numbers and 1,000 random numbers.  

##A. Explain the differences among these figures. Do they all indicate that the data are white noise?  

Yes because because there is no correlation from one point to the next and all the values fall within the bounds of ± 2/√T.  

##B. Why are the critical values at different distances from the mean of zero? Why are the autocorrelations different in each figure when they each refer to white noise?

It can be assumed that some observations can break the critical values while stil being white noise. The more numbers in a series of data the less fluctuation we are likely to observe in the variation.  

#8.2. A classic example of a non-stationary series is the daily closing IBM stock price series (data set ibmclose). Use R to plot the daily closing prices for IBM stock and the ACF and PACF. Explain how each plot shows that the series is non-stationary and should be differenced.

```{r}
data(ibmclose)
tsdisplay(ibmclose)
```
As you will see from the scatterplot of IBM closing prices, there are clear trends in the data over periods of time. This suggests it is non-stationary, since stationary data would only show random fluctuations.  
In the ACF plot, we again see a clear trend in the data, with large spikes in autocorrelations way above the bounds of the critical values within which you'd expect nearly all values if it were white noise.  
In the PACF plot, we see a significant spike at lag of 1, well outside the critical values.  


#8.7: Consider wmurders, the number of women murdered each year (per 100,000 standard population) in the United States.
```{r}
data(wmurders)
```

##A. By studying appropriate graphs of the series in R, find an appropriate ARIMA(p,d,q ) model for these data.  

```{r}
tsdisplay(wmurders)
wmurders %>% diff() %>% ur.kpss() %>% summary()
wmurders %>% diff(differences=2) %>% ur.kpss() %>% summary()
ndiffs(wmurders)
```
These queries suggest an ARIMA(0,2,2) model since the PACF decays exponentially over time and the ACF spikes at lag of 2 and the queries above suggest a d of 2.  

##B. Should you include a constant in the model? Explain.

No, there is no drastic change in the average.  

##C. Write this model in terms of the backshift operator.

(1 ??? B)^2 yt  = (1 + theta1*B + theta2*B^2)*et  

##D. Fit the model using R and examine the residuals. Is the model satisfactory?  
```{r}
fit <- Arima(wmurders, order = c(0,2,2))
plot(fit$residuals)
```
Yes, the model seems satisfactory. The residual are somewhat normally distributed for both the model we selected and all the values in the ACF plot fall within ± 2/√T.  


##E. Forecast three times ahead. Check your forecasts by hand to make sure that you know how they have been calculated.  
```{r}
forecast <- forecast(fit, h=3)
forecast
```
##F. Create a plot of the series with forecasts and prediction intervals for the next three periods shown.  

```{r}
plot(forecast)
```
##G. Does auto.arima() give the same model you have chosen? If not, which model do you think is better?  

```{r}
fit_g <- auto.arima(wmurders)
fit_g
```

Using the auto.arima models gives us an ARIMA (1,2,1) model. The ACF actually returns better results than the model we selected. For example, the AIC value is slightly lower.  

#8.12: For the mcopper data:  

```{r}
data(mcopper)
plot(mcopper)
```
##A. if necessary, find a suitable Box-Cox transformation for the data;  

By reviewing the scatterplot it appears there is not a huge change in variance for this data set, so it does not require a transformation. Below we show a Box-Cox transformation to see what it would look like.  

```{r}
lamda <- BoxCox.lambda(mcopper)
mcopper_bcx <- BoxCox(mcopper, lambda = lamda)
tsdisplay(mcopper_bcx)
```

##B. fit a suitable ARIMA model to the transformed data using auto.arima();  

```{r}
fit <- auto.arima(mcopper, trace = TRUE, ic ="aic", lambda = lamda)
fit
```

The auto.arima function returns an ARIMA (0,1,1) model.   

##C. try some other plausible models by experimenting with the orders chosen;  

```{r}
fit_2 <- Arima(mcopper, order = c(2,1,2), lambda = lamda)
fit_3 <- Arima(mcopper, order = c(0,1,1), lambda = lamda)
accuracy(fit)
accuracy(fit_2)
accuracy(fit_3)
```
Above we try a few different models.  We select for final analysis a Arima(2,1,2) model - aka fit_2.  


##D. choose what you think is the best model and check the residual diagnostics;  

Here we test the residuals for our best model fit_2. The residuals are very normally distributed, and all the values fall within the critical value bounds on the ACF plot.  

```{r}
plot(fit_2$residuals)
```

##E. produce forecasts of your fitted model. Do the forecasts look reasonable?  
```{r}
forecast <- forecast(fit_2, h=10)
plot(forecast)
```
The forecast looks reasonable.  

forecasts by manual calculation:  
formula for ARIMA(0,2,2):  
(1 - B)^2*yt = (1 + theta1*B + theta2*B^2)*et  (re: chapter 8.8)  
or yt - 2yt-1 + yt-2 = (1 + theta1*B + theta2*B^2)*et  (re: chapter 8.2)  
or, yt = 2yt-1 - yt-2 + et+theta1*et-1 + theta2*et-2  

```{r}
wmurder_fit$model$theta
```
here, theta1 = -1.0181  
      theta2 = 0.1467
      
```{r}
e <- wmurder_fit$residuals
T <- length(wmurders)
```
 
So forecast1 = 2yT - yT-1 + eT+1 + theta1*eT + theta2*eT-1  (replacing t with T+1)  
Since we do not know the value of eT+1, it will be replaced with zero (re: chapter 8.2 )  
By replacing values for theta1, theta2, e and T we get  

```{r}
forecast1 <- 2*wmurders[T] - wmurders[T-1] - 1.0181*e[T]  + 0.1467*e[T-1]
forecast2 <- 2*forecast1 - wmurders[T]  + 0.1467*e[T-1]
forecast3 <- 2*forecast2 - forecast1
```
So the forecasts are

```{r}
c(forecast1,forecast2,forecast3)
```


##F. compare the results with what you would obtain using ets() (with no transformation).  

```{r}
fit_4 <- ets(mcopper); fit_4
forecast2 <- forecast(fit_4, h=10)
plot(forecast2)
```

Unlike our Arima models (both the model we selected and the Auto Arima model) which forecast values staying level, the ETS model forecasts falling values. While the upper bounds of each condidence interval appear similar for each model, the ETS model has lower bounds within the confidence intervals.  
